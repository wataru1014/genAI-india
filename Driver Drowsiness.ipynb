{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "547bb763-2f2f-421b-ac35-e2dd0c46c245",
   "metadata": {},
   "source": [
    "## ドライバーの眠気検知"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ed06a1",
   "metadata": {},
   "source": [
    "### 前処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af052844",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout,BatchNormalization\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd51b16",
   "metadata": {},
   "source": [
    "## データセットの確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a7e6ee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of drowsy images in the folder: 22348\n",
      "Number of non drowsy images in the folder: 19445\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "# スクリプトがあるディレクトリのパスを取得\n",
    "base_dir = os.getcwd()\n",
    "\n",
    "# 相対パスを設定\n",
    "drowsy_path = os.path.join(base_dir, 'Driver Drowsiness Dataset (DDD)', 'Drowsy')\n",
    "non_path = os.path.join(base_dir, 'Driver Drowsiness Dataset (DDD)', 'Non Drowsy')\n",
    "\n",
    "image_extension = '.png'\n",
    "\n",
    "total_drowsy = len([f for f in os.listdir(drowsy_path) if f.lower().endswith(image_extension)])\n",
    "total_non = len([f for f in os.listdir(non_path) if f.lower().endswith(image_extension)])\n",
    "\n",
    "print(f\"Number of drowsy images in the folder: {total_drowsy}\")\n",
    "print(f\"Number of non drowsy images in the folder: {total_non}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41e978f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import dlib\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')\n",
    "face_detector = dlib.get_frontal_face_detector()\n",
    "eye_detector = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62cb4726",
   "metadata": {},
   "source": [
    "## 前処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e9f6ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EAR計算関数\n",
    "from scipy.spatial import distance\n",
    "def get_eye_aspect_ratio(eye):\n",
    "    A = distance.euclidean(eye[1], eye[5])\n",
    "    B = distance.euclidean(eye[2], eye[4])\n",
    "    C = distance.euclidean(eye[0], eye[3])\n",
    "    return (A + B) / (2.0 * C)\n",
    "\n",
    "def get_landmarks(shape):\n",
    "    return [(shape.part(i).x, shape.part(i).y) for i in range(shape.num_parts)]\n",
    "eye_closed_time = None  # 目を閉じた時刻を追跡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49fb2b07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "データ数: 16086\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import dlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# dlibの顔検出器とランドマーク予測器\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n",
    "\n",
    "# 左右のEARスケーラー\n",
    "scaler_left = StandardScaler()\n",
    "scaler_right = StandardScaler()\n",
    "\n",
    "# 68ランドマークにおける目のインデックス\n",
    "LEFT_EYE_POINTS = list(range(36, 42))\n",
    "RIGHT_EYE_POINTS = list(range(42, 48))\n",
    "\n",
    "def get_eye_aspect_ratio(eye):\n",
    "    # EAR計算\n",
    "    A = np.linalg.norm(eye[1] - eye[5])\n",
    "    B = np.linalg.norm(eye[2] - eye[4])\n",
    "    C = np.linalg.norm(eye[0] - eye[3])\n",
    "    ear = (A + B) / (2.0 * C)\n",
    "    return ear\n",
    "\n",
    "def crop_eye_region(image, eye_points, padding=5):\n",
    "    x_min = np.min(eye_points[:, 0]) - padding\n",
    "    x_max = np.max(eye_points[:, 0]) + padding\n",
    "    y_min = np.min(eye_points[:, 1]) - padding\n",
    "    y_max = np.max(eye_points[:, 1]) + padding\n",
    "\n",
    "    # 範囲が画像をはみ出さないようにクリップ\n",
    "    x_min = max(x_min, 0)\n",
    "    y_min = max(y_min, 0)\n",
    "    x_max = min(x_max, image.shape[1])\n",
    "    y_max = min(y_max, image.shape[0])\n",
    "\n",
    "    eye_img = image[y_min:y_max, x_min:x_max]\n",
    "    return cv2.resize(eye_img, (64, 64))  # 学習用にリサイズ\n",
    "\n",
    "def extract_eye_data_from_image(img_path):\n",
    "    img = cv2.imread(img_path)\n",
    "    if img is None:\n",
    "        return None, None, None, None\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    faces = detector(gray)\n",
    "    if len(faces) == 0:\n",
    "        return None, None, None, None\n",
    "\n",
    "    for face in faces:\n",
    "        landmarks = predictor(gray, face)\n",
    "\n",
    "        left_eye_coords = np.array([(landmarks.part(n).x, landmarks.part(n).y) for n in LEFT_EYE_POINTS])\n",
    "        right_eye_coords = np.array([(landmarks.part(n).x, landmarks.part(n).y) for n in RIGHT_EYE_POINTS])\n",
    "\n",
    "        left_ear = get_eye_aspect_ratio(left_eye_coords)\n",
    "        right_ear = get_eye_aspect_ratio(right_eye_coords)\n",
    "\n",
    "        left_eye_img = crop_eye_region(img, left_eye_coords)\n",
    "        right_eye_img = crop_eye_region(img, right_eye_coords)\n",
    "\n",
    "        # 横に結合\n",
    "        both_eyes_img = np.hstack((left_eye_img, right_eye_img))  # shape (64, 128, 3)\n",
    "        #print(both_eyes_img.shape)\n",
    "\n",
    "\n",
    "        return both_eyes_img, left_ear, right_ear, img\n",
    "\n",
    "    return None, None, None, None\n",
    "\n",
    "# ===== データセット作成 =====\n",
    "data = []\n",
    "random.seed(42)\n",
    "\n",
    "for folder, label in [(drowsy_path, \"Drowsy\"), (non_path, \"Non Drowsy\")]:\n",
    "    files = [f for f in os.listdir(folder) if f.lower().endswith('.png')]\n",
    "    random.shuffle(files)\n",
    "    for file in files[:10000]:\n",
    "        img_path = os.path.join(folder, file)\n",
    "        both_img, left_ear, right_ear, _ = extract_eye_data_from_image(img_path)\n",
    "        if both_img is not None:\n",
    "            total_ear = (left_ear+right_ear)/2\n",
    "            data.append({\n",
    "                \"both_img\": both_img,\n",
    "                \"total EAR\": total_ear,\n",
    "                \"label\": label\n",
    "            })\n",
    "\n",
    "# DataFrame化\n",
    "df = pd.DataFrame(data)\n",
    "print(f\"データ数: {len(df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9c5c89",
   "metadata": {},
   "source": [
    "### データセットの準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a17dd346",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = (64, 128)\n",
    "\n",
    "both_img = []\n",
    "total_ears = []\n",
    "labels = []\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    both_eye_img = cv2.cvtColor(row[\"both_img\"], cv2.COLOR_BGR2RGB)\n",
    "    both_eye_img = both_eye_img / 255.0\n",
    "\n",
    "    both_img.append(both_eye_img)\n",
    "    total_ears.append(row[\"total EAR\"])\n",
    "    labels.append(1 if row[\"label\"] == \"Drowsy\" else 0)\n",
    "\n",
    "both_img = np.array(both_img,dtype=np.float32)\n",
    "total_ears = np.array(total_ears,dtype=np.float32).reshape(-1, 1)\n",
    "labels = np.array(labels, dtype=np.int32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fafc0684",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_img_train, X_img_test, X_ear_train, X_ear_test, y_train, y_test = train_test_split(\n",
    "    both_img, total_ears, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# EARのスケーリング\n",
    "scaler_ear = StandardScaler()\n",
    "\n",
    "train_ear_scaled = scaler_ear.fit_transform(X_ear_train.reshape(-1, 1))\n",
    "\n",
    "\n",
    "val_ear_scaled = scaler_ear.transform(X_ear_test.reshape(-1, 1))\n",
    "\n",
    "# --- EAR差強調 ---\n",
    "alpha = 3.0  # 差を強調する係数（調整可能）\n",
    "train_ear_scaled *= alpha\n",
    "val_ear_scaled *= alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa22ee5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class LossGapEarlyStopping(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, threshold=0.0001):\n",
    "        super().__init__()\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        train_loss = logs.get('loss')\n",
    "        val_loss = logs.get('val_loss')\n",
    "        if train_loss is None or val_loss is None:\n",
    "            return\n",
    "        \n",
    "        gap = abs(val_loss - train_loss)\n",
    "        print(f\"Epoch {epoch+1}: loss gap = {gap:.6f}\")\n",
    "        \n",
    "        if gap < self.threshold:\n",
    "            print(f\"Stopping early: loss gap {gap:.6f} < threshold {self.threshold}\")\n",
    "            self.model.stop_training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f3915e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, Input\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Lambda, Concatenate\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# ===== CNNブランチ（結合画像用） =====\n",
    "def build_cnn_branch():\n",
    "    img_input = Input(shape=(64, 128, 3))  # 横に結合された画像\n",
    "    x = Conv2D(32, (3,3), activation='relu')(img_input)\n",
    "    x = MaxPooling2D()(x)\n",
    "    x = Conv2D(64, (3,3), activation='relu')(x)\n",
    "    x = MaxPooling2D()(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Lambda(lambda z: z * 0.3)(x)  # CNN出力スケーリング\n",
    "    return img_input, x\n",
    "\n",
    "both_img_input, img_features = build_cnn_branch()\n",
    "\n",
    "# ===== total EAR入力 =====\n",
    "ear_input = Input(shape=(1,), name=\"total_ear_input\")\n",
    "ear_features = Dense(16, activation='relu')(ear_input)\n",
    "ear_features = Dense(32, activation='relu')(ear_features)\n",
    "ear_features = Lambda(lambda z: z * 10.0)(ear_features)  # スケーリング\n",
    "\n",
    "# ===== 特徴量結合 =====\n",
    "merged = Concatenate()([img_features, ear_features])\n",
    "merged = Dense(128, activation='relu')(merged)\n",
    "output = Dense(1, activation='sigmoid')(merged)\n",
    "\n",
    "# ===== モデル定義 =====\n",
    "model = models.Model(\n",
    "    inputs=[both_img_input, ear_input],\n",
    "    outputs=output\n",
    ")\n",
    "\n",
    "# ===== コールバック =====\n",
    "checkpoint = ModelCheckpoint(\n",
    "    'best_model_combined_totalEAR.keras',\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    mode='min',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82bdf2e7",
   "metadata": {},
   "source": [
    "### モデルの構築"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "87089c79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/moriwataru/.pyenv/versions/3.11.4/lib/python3.11/site-packages/keras/src/models/functional.py:225: UserWarning: The structure of `inputs` doesn't match the expected structure: ['keras_tensor_29', 'total_ear_input']. Received: the structure of inputs=('*', '*')\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m402/403\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - accuracy: 0.6973 - loss: 0.8546\n",
      "Epoch 1: val_loss improved from inf to 0.07304, saving model to best_model_combined_totalEAR.keras\n",
      "\u001b[1m403/403\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 157ms/step - accuracy: 0.6980 - loss: 0.8529 - val_accuracy: 0.9748 - val_loss: 0.0730\n",
      "Epoch 2/10\n",
      "\u001b[1m402/403\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - accuracy: 0.9675 - loss: 0.1452\n",
      "Epoch 2: val_loss improved from 0.07304 to 0.02835, saving model to best_model_combined_totalEAR.keras\n",
      "\u001b[1m403/403\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 165ms/step - accuracy: 0.9675 - loss: 0.1450 - val_accuracy: 0.9913 - val_loss: 0.0284\n",
      "Epoch 3/10\n",
      "\u001b[1m402/403\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - accuracy: 0.9893 - loss: 0.0560\n",
      "Epoch 3: val_loss did not improve from 0.02835\n",
      "\u001b[1m403/403\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 159ms/step - accuracy: 0.9893 - loss: 0.0560 - val_accuracy: 0.9810 - val_loss: 0.0453\n",
      "Epoch 4/10\n",
      "\u001b[1m402/403\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - accuracy: 0.9930 - loss: 0.0299\n",
      "Epoch 4: val_loss improved from 0.02835 to 0.01853, saving model to best_model_combined_totalEAR.keras\n",
      "\u001b[1m403/403\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 164ms/step - accuracy: 0.9930 - loss: 0.0299 - val_accuracy: 0.9925 - val_loss: 0.0185\n",
      "Epoch 5/10\n",
      "\u001b[1m402/403\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 173ms/step - accuracy: 0.9950 - loss: 0.0232\n",
      "Epoch 5: val_loss improved from 0.01853 to 0.01107, saving model to best_model_combined_totalEAR.keras\n",
      "\u001b[1m403/403\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 184ms/step - accuracy: 0.9950 - loss: 0.0232 - val_accuracy: 0.9966 - val_loss: 0.0111\n",
      "Epoch 6/10\n",
      "\u001b[1m403/403\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 193ms/step - accuracy: 0.9949 - loss: 0.0243\n",
      "Epoch 6: val_loss did not improve from 0.01107\n",
      "\u001b[1m403/403\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 204ms/step - accuracy: 0.9949 - loss: 0.0243 - val_accuracy: 0.9901 - val_loss: 0.0484\n",
      "Epoch 7/10\n",
      "\u001b[1m403/403\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - accuracy: 0.9907 - loss: 0.0409\n",
      "Epoch 7: val_loss improved from 0.01107 to 0.00799, saving model to best_model_combined_totalEAR.keras\n",
      "\u001b[1m403/403\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 169ms/step - accuracy: 0.9907 - loss: 0.0408 - val_accuracy: 0.9981 - val_loss: 0.0080\n",
      "Epoch 8/10\n",
      "\u001b[1m402/403\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 163ms/step - accuracy: 0.9979 - loss: 0.0092\n",
      "Epoch 8: val_loss did not improve from 0.00799\n",
      "\u001b[1m403/403\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 174ms/step - accuracy: 0.9979 - loss: 0.0092 - val_accuracy: 0.9960 - val_loss: 0.0126\n",
      "Epoch 9/10\n",
      "\u001b[1m402/403\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.9965 - loss: 0.0165\n",
      "Epoch 9: val_loss improved from 0.00799 to 0.00730, saving model to best_model_combined_totalEAR.keras\n",
      "\u001b[1m403/403\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 166ms/step - accuracy: 0.9965 - loss: 0.0165 - val_accuracy: 0.9981 - val_loss: 0.0073\n",
      "Epoch 10/10\n",
      "\u001b[1m402/403\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 163ms/step - accuracy: 0.9998 - loss: 0.0018\n",
      "Epoch 10: val_loss improved from 0.00730 to 0.00694, saving model to best_model_combined_totalEAR.keras\n",
      "\u001b[1m403/403\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 173ms/step - accuracy: 0.9998 - loss: 0.0018 - val_accuracy: 0.9978 - val_loss: 0.0069\n",
      "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 66ms/step - accuracy: 0.9978 - loss: 0.0053\n",
      "Validation Accuracy: 0.9978\n"
     ]
    }
   ],
   "source": [
    "# ===== 学習 =====\n",
    "history = model.fit(\n",
    "    [X_img_train, train_ear_scaled],  # 入力は [画像, EAR]\n",
    "    y_train,\n",
    "    validation_data=(\n",
    "        [X_img_test, val_ear_scaled],\n",
    "        y_test\n",
    "    ),\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    callbacks=[checkpoint],\n",
    "    class_weight={0: 1, 1: 5}  # Drowsyクラスを重み付け\n",
    ")\n",
    "\n",
    "# ===== 評価 =====\n",
    "loss, acc = model.evaluate(\n",
    "    [X_img_test, val_ear_scaled],\n",
    "    y_test\n",
    ")\n",
    "print(f\"Validation Accuracy: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630a17d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/moriwataru/.pyenv/versions/3.11.4/lib/python3.11/site-packages/keras/src/models/functional.py:225: UserWarning: The structure of `inputs` doesn't match the expected structure: ['keras_tensor_29', 'total_ear_input']. Received: the structure of inputs=('*', '*')\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m現在のセルまたは前のセルでコードを実行中に、カーネル (Kernel) がクラッシュしました。\n",
      "\u001b[1;31mエラーの原因を特定するには、セル内のコードを確認してください。\n",
      "\u001b[1;31m詳細については<a href='https://aka.ms/vscodeJupyterKernelCrash'>こちら</a>をクリックします。\n",
      "\u001b[1;31m詳細については、Jupyter <a href='command:jupyter.viewOutput'>ログ</a> を参照してください。"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import dlib\n",
    "import numpy as np\n",
    "from scipy.spatial import distance as dist\n",
    "from collections import deque\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# ===== EAR計算関数 =====\n",
    "def eye_aspect_ratio(eye):\n",
    "    A = dist.euclidean(eye[1], eye[5])\n",
    "    B = dist.euclidean(eye[2], eye[4])\n",
    "    C = dist.euclidean(eye[0], eye[3])\n",
    "    ear = (A + B) / (2.0 * C)\n",
    "    return ear\n",
    "\n",
    "# ===== 移動平均クラス =====\n",
    "class MovingAverage:\n",
    "    def __init__(self, size=5):\n",
    "        self.values = deque(maxlen=size)\n",
    "    def update(self, val):\n",
    "        self.values.append(val)\n",
    "        return np.mean(self.values) if self.values else val\n",
    "\n",
    "\n",
    "# ===== Dlib準備 =====\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n",
    "\n",
    "#model = load_model(\"best_model_combined_totalEAR.keras\",safe_mode=False)\n",
    "\n",
    "# 目のランドマークインデックス\n",
    "LEFT_EYE_IDX = list(range(36, 42))\n",
    "RIGHT_EYE_IDX = list(range(42, 48))\n",
    "\n",
    "# ===== 移動平均用インスタンス =====\n",
    "ma_left = MovingAverage(size=3)\n",
    "ma_right = MovingAverage(size=3)\n",
    "\n",
    "# ===== カメラ開始 =====\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = detector(gray, 0)\n",
    "    \n",
    "    for face in faces:\n",
    "        shape = predictor(gray, face)\n",
    "        shape_np = np.zeros((68, 2), dtype=\"int\")\n",
    "        for i in range(68):\n",
    "            shape_np[i] = (shape.part(i).x, shape.part(i).y)\n",
    "        \n",
    "        # 左右の目座標\n",
    "        left_eye_points = shape_np[LEFT_EYE_IDX]\n",
    "        right_eye_points = shape_np[RIGHT_EYE_IDX]\n",
    "        \n",
    "        # EAR計算（移動平均付き）\n",
    "        ear_left = ma_left.update(eye_aspect_ratio(left_eye_points))\n",
    "        ear_right = ma_right.update(eye_aspect_ratio(right_eye_points))\n",
    "        total_ear = (ear_left + ear_right) / 2\n",
    "        \n",
    "        # total_ear をスケーリング（学習時の平均と標準偏差を適用）\n",
    "        total_ear_scaled = scaler_ear.transform([[total_ear]])[0][0]*alpha\n",
    "        ear_input = np.array([[total_ear_scaled]],dtype=np.float32)\n",
    "\n",
    "        # 左目切り出し\n",
    "        lx, ly, lw, lh = cv2.boundingRect(left_eye_points)\n",
    "        left_eye_img = frame[ly:ly+lh, lx:lx+lw]\n",
    "        left_eye_img = cv2.resize(left_eye_img, (64, 64))\n",
    "        \n",
    "        # 右目切り出し\n",
    "        rx, ry, rw, rh = cv2.boundingRect(right_eye_points)\n",
    "        right_eye_img = frame[ry:ry+rh, rx:rx+rw]\n",
    "        right_eye_img = cv2.resize(right_eye_img, (64, 64))\n",
    "\n",
    "        # 正規化 & 次元追加\n",
    "        left_eye_img = left_eye_img.astype(\"float32\") / 255.0\n",
    "        right_eye_img = right_eye_img.astype(\"float32\") / 255.0\n",
    "        \n",
    "        # 横に結合（128x64x3 の画像）\n",
    "        combined_images = np.hstack((left_eye_img, right_eye_img))\n",
    "        combined_images = np.expand_dims(combined_images, axis=0)\n",
    "\n",
    "        # モデル予測\n",
    "        pred = model.predict([combined_images, ear_input], verbose=0)[0][0]\n",
    "        \n",
    "        # 判定\n",
    "        label = \"Drowsy\" if pred > 0.2 else \"Non Drowsy\"\n",
    "        color = (0, 0, 255) if label == \"Drowsy\" else (0, 255, 0)\n",
    "        \n",
    "        # 表示\n",
    "        cv2.putText(frame, f\"{label} ({pred:.2f})\", (50, 50),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1.2, color, 2)\n",
    "        if label == \"Drowsy\":\n",
    "            cv2.putText(frame, \"!!! WARNING !!!\", (100, 100),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0, 0, 255), 3)\n",
    "    \n",
    "    cv2.imshow(\"Drowsiness Detection\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55664cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
