{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "547bb763-2f2f-421b-ac35-e2dd0c46c245",
   "metadata": {},
   "source": [
    "## ドライバーの眠気検知"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ed06a1",
   "metadata": {},
   "source": [
    "### 前処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af052844",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout,BatchNormalization\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd51b16",
   "metadata": {},
   "source": [
    "## データセットの確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a7e6ee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of drowsy images in the folder: 22348\n",
      "Number of non drowsy images in the folder: 19445\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "# スクリプトがあるディレクトリのパスを取得\n",
    "base_dir = os.getcwd()\n",
    "\n",
    "# 相対パスを設定\n",
    "drowsy_path = os.path.join(base_dir, 'Driver Drowsiness Dataset (DDD)', 'Drowsy')\n",
    "non_path = os.path.join(base_dir, 'Driver Drowsiness Dataset (DDD)', 'Non Drowsy')\n",
    "#drowsy_path = os.path.join(base_dir, 'dataset', 'Drowsy')\n",
    "#non_path = os.path.join(base_dir, 'dataset', 'Non Drowsy')\n",
    "\n",
    "image_extension = '.png'\n",
    "\n",
    "total_drowsy = len([f for f in os.listdir(drowsy_path) if f.lower().endswith(image_extension)])\n",
    "total_non = len([f for f in os.listdir(non_path) if f.lower().endswith(image_extension)])\n",
    "\n",
    "print(f\"Number of drowsy images in the folder: {total_drowsy}\")\n",
    "print(f\"Number of non drowsy images in the folder: {total_non}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62cb4726",
   "metadata": {},
   "source": [
    "## 前処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6d8d8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import dlib\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')\n",
    "face_detector = dlib.get_frontal_face_detector()\n",
    "eye_detector = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "15ff6d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EAR計算関数\n",
    "from scipy.spatial import distance\n",
    "def get_eye_aspect_ratio(eye):\n",
    "    A = distance.euclidean(eye[1], eye[5])\n",
    "    B = distance.euclidean(eye[2], eye[4])\n",
    "    C = distance.euclidean(eye[0], eye[3])\n",
    "    return (A + B) / (2.0 * C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "16a46061",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_landmarks(shape):\n",
    "    return [(shape.part(i).x, shape.part(i).y) for i in range(shape.num_parts)]\n",
    "eye_closed_time = None  # 目を閉じた時刻を追跡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1e9f6ce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "データ数: 27\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import dlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# dlibの顔検出器とランドマーク予測器\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n",
    "\n",
    "# 左右のEARスケーラー\n",
    "scaler_left = StandardScaler()\n",
    "scaler_right = StandardScaler()\n",
    "\n",
    "# 68ランドマークにおける目のインデックス\n",
    "LEFT_EYE_POINTS = list(range(36, 42))\n",
    "RIGHT_EYE_POINTS = list(range(42, 48))\n",
    "\n",
    "def get_eye_aspect_ratio(eye):\n",
    "    # EAR計算\n",
    "    A = np.linalg.norm(eye[1] - eye[5])\n",
    "    B = np.linalg.norm(eye[2] - eye[4])\n",
    "    C = np.linalg.norm(eye[0] - eye[3])\n",
    "    ear = (A + B) / (2.0 * C)\n",
    "    return ear\n",
    "\n",
    "def crop_eye_region(image, eye_points, padding=5):\n",
    "    x_min = np.min(eye_points[:, 0]) - padding\n",
    "    x_max = np.max(eye_points[:, 0]) + padding\n",
    "    y_min = np.min(eye_points[:, 1]) - padding\n",
    "    y_max = np.max(eye_points[:, 1]) + padding\n",
    "\n",
    "    # 範囲が画像をはみ出さないようにクリップ\n",
    "    x_min = max(x_min, 0)\n",
    "    y_min = max(y_min, 0)\n",
    "    x_max = min(x_max, image.shape[1])\n",
    "    y_max = min(y_max, image.shape[0])\n",
    "\n",
    "    eye_img = image[y_min:y_max, x_min:x_max]\n",
    "    return cv2.resize(eye_img, (64, 64))  # 学習用にリサイズ\n",
    "\n",
    "def extract_eye_data_from_image(img_path):\n",
    "    img = cv2.imread(img_path)\n",
    "    if img is None:\n",
    "        return None, None, None, None, None\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    faces = detector(gray)\n",
    "    if len(faces) == 0:\n",
    "        return None, None, None, None, None\n",
    "\n",
    "    for face in faces:\n",
    "        landmarks = predictor(gray, face)\n",
    "\n",
    "        left_eye_coords = np.array([(landmarks.part(n).x, landmarks.part(n).y) for n in LEFT_EYE_POINTS])\n",
    "        right_eye_coords = np.array([(landmarks.part(n).x, landmarks.part(n).y) for n in RIGHT_EYE_POINTS])\n",
    "\n",
    "        left_ear = get_eye_aspect_ratio(left_eye_coords)\n",
    "        right_ear = get_eye_aspect_ratio(right_eye_coords)\n",
    "\n",
    "        left_eye_img = crop_eye_region(img, left_eye_coords)\n",
    "        right_eye_img = crop_eye_region(img, right_eye_coords)\n",
    "\n",
    "        return left_eye_img, right_eye_img, left_ear, right_ear, img\n",
    "\n",
    "    return None, None, None, None, None\n",
    "\n",
    "# ===== データセット作成 =====\n",
    "data = []\n",
    "random.seed(42)\n",
    "\n",
    "for folder, label in [(drowsy_path, \"Drowsy\"), (non_path, \"Non Drowsy\")]:\n",
    "    files = [f for f in os.listdir(folder) if f.lower().endswith('.png')]\n",
    "    random.shuffle(files)\n",
    "    for file in files[:15]:\n",
    "        img_path = os.path.join(folder, file)\n",
    "        left_eye_img, right_eye_img, left_ear, right_ear, _ = extract_eye_data_from_image(img_path)\n",
    "        if left_eye_img is not None:\n",
    "            data.append({\n",
    "                \"left_eye_img\": left_eye_img,\n",
    "                \"right_eye_img\": right_eye_img,\n",
    "                \"Left EAR\": left_ear,\n",
    "                \"Right EAR\": right_ear,\n",
    "                \"label\": label\n",
    "            })\n",
    "\n",
    "# DataFrame化\n",
    "df = pd.DataFrame(data)\n",
    "print(f\"データ数: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "31dc0d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== データの準備（左右別々の目画像バージョン） =====\n",
    "IMG_SIZE = (64, 64)\n",
    "\n",
    "left_images = []\n",
    "right_images = []\n",
    "left_ears = []\n",
    "right_ears = []\n",
    "labels = []\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    # 左目画像（BGR → RGB → サイズ統一）\n",
    "    left_eye_img = cv2.cvtColor(row[\"left_eye_img\"], cv2.COLOR_BGR2RGB)\n",
    "    left_eye_img = cv2.resize(left_eye_img, IMG_SIZE) / 255.0  # 正規化\n",
    "\n",
    "    # 右目画像\n",
    "    right_eye_img = cv2.cvtColor(row[\"right_eye_img\"], cv2.COLOR_BGR2RGB)\n",
    "    right_eye_img = cv2.resize(right_eye_img, IMG_SIZE) / 255.0\n",
    "\n",
    "    left_images.append(left_eye_img)\n",
    "    right_images.append(right_eye_img)\n",
    "    left_ears.append(row[\"Left EAR\"])\n",
    "    right_ears.append(row[\"Right EAR\"])\n",
    "    labels.append(1 if row[\"label\"] == \"Drowsy\" else 0)\n",
    "\n",
    "# NumPy配列に変換\n",
    "left_images = np.array(left_images, dtype=np.float32)\n",
    "right_images = np.array(right_images, dtype=np.float32)\n",
    "lears = np.array(left_ears, dtype=np.float32).reshape(-1, 1)\n",
    "rears = np.array(right_ears, dtype=np.float32).reshape(-1, 1)\n",
    "labels = np.array(labels, dtype=np.int32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9d3852bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 訓練・テスト分割（左右目画像とEAR値を個別に）\n",
    "X_left_train, X_left_test, \\\n",
    "X_right_train, X_right_test, \\\n",
    "X_lear_train, X_lear_test, \\\n",
    "X_rear_train, X_rear_test, \\\n",
    "y_train, y_test = train_test_split(\n",
    "    left_images, right_images, lears, rears, labels, \n",
    "    test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# EARのスケーリング\n",
    "scaler_left = StandardScaler()\n",
    "scaler_right = StandardScaler()\n",
    "\n",
    "train_ear_left_scaled = scaler_left.fit_transform(X_lear_train.reshape(-1, 1))\n",
    "train_ear_right_scaled = scaler_right.fit_transform(X_rear_train.reshape(-1, 1))\n",
    "\n",
    "val_ear_left_scaled = scaler_left.transform(X_lear_test.reshape(-1, 1))\n",
    "val_ear_right_scaled = scaler_right.transform(X_rear_test.reshape(-1, 1))\n",
    "\n",
    "# --- EAR差強調 ---\n",
    "alpha = 3.0  # 差を強調する係数（調整可能）\n",
    "train_ear_left_scaled *= alpha\n",
    "train_ear_right_scaled *= alpha\n",
    "val_ear_left_scaled *= alpha\n",
    "val_ear_right_scaled *= alpha\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f4372bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class LossGapEarlyStopping(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, threshold=0.0001):\n",
    "        super().__init__()\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        train_loss = logs.get('loss')\n",
    "        val_loss = logs.get('val_loss')\n",
    "        if train_loss is None or val_loss is None:\n",
    "            return\n",
    "        \n",
    "        gap = abs(val_loss - train_loss)\n",
    "        print(f\"Epoch {epoch+1}: loss gap = {gap:.6f}\")\n",
    "        \n",
    "        if gap < self.threshold:\n",
    "            print(f\"Stopping early: loss gap {gap:.6f} < threshold {self.threshold}\")\n",
    "            self.model.stop_training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2cb67b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, Input\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Lambda, Concatenate\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# ===== CNNブロックを関数化（左右で共有） =====\n",
    "def build_cnn_branch():\n",
    "    img_input = Input(shape=(64, 64, 3))\n",
    "    x = Conv2D(32, (3,3), activation='relu')(img_input)\n",
    "    x = MaxPooling2D()(x)\n",
    "    x = Conv2D(64, (3,3), activation='relu')(x)\n",
    "    x = MaxPooling2D()(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Lambda(lambda z: z * 0.3)(x)  # CNN出力をスケーリング\n",
    "    return img_input, x\n",
    "\n",
    "# ===== 左右画像入力 =====\n",
    "left_img_input, left_features = build_cnn_branch()\n",
    "right_img_input, right_features = build_cnn_branch()\n",
    "\n",
    "# ===== EAR入力（左） =====\n",
    "ear_left_input = Input(shape=(1,), name=\"ear_left_input\")\n",
    "e_left = Dense(16, activation='relu')(ear_left_input)\n",
    "e_left = Dense(32, activation='relu')(e_left)\n",
    "e_left = Lambda(lambda z: z * 7.0)(e_left)  # 左EARスケーリング\n",
    "\n",
    "# ===== EAR入力（右） =====\n",
    "ear_right_input = Input(shape=(1,), name=\"ear_right_input\")\n",
    "e_right = Dense(16, activation='relu')(ear_right_input)\n",
    "e_right = Dense(32, activation='relu')(e_right)\n",
    "e_right = Lambda(lambda z: z * 7.0)(e_right)  # 右EARスケーリング\n",
    "\n",
    "# ===== 特徴量結合 =====\n",
    "merged = Concatenate()([left_features, right_features, e_left, e_right])\n",
    "merged = Dense(128, activation='relu')(merged)\n",
    "output = Dense(1, activation='sigmoid')(merged)\n",
    "\n",
    "# ===== モデル定義 =====\n",
    "model = models.Model(\n",
    "    inputs=[left_img_input, right_img_input, ear_left_input, ear_right_input],\n",
    "    outputs=output\n",
    ")\n",
    "\n",
    "# ===== コールバック =====\n",
    "checkpoint = ModelCheckpoint(\n",
    "    'best_model_human_15.keras',\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    mode='min',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ee273611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/moriwataru/.pyenv/versions/3.11.4/lib/python3.11/site-packages/keras/src/models/functional.py:225: UserWarning: The structure of `inputs` doesn't match the expected structure: ['keras_tensor_674', 'keras_tensor_683', 'ear_left_input', 'ear_right_input']. Received: the structure of inputs=('*', '*', '*', '*')\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m605/605\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - accuracy: 0.7547 - loss: 0.7120\n",
      "Epoch 1: val_loss improved from inf to 0.06442, saving model to best_model_human_15.keras\n",
      "\u001b[1m605/605\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 125ms/step - accuracy: 0.7549 - loss: 0.7115 - val_accuracy: 0.9820 - val_loss: 0.0644\n",
      "Epoch 2/8\n",
      "\u001b[1m605/605\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - accuracy: 0.9718 - loss: 0.1149\n",
      "Epoch 2: val_loss improved from 0.06442 to 0.03817, saving model to best_model_human_15.keras\n",
      "\u001b[1m605/605\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 128ms/step - accuracy: 0.9718 - loss: 0.1149 - val_accuracy: 0.9872 - val_loss: 0.0382\n",
      "Epoch 3/8\n",
      "\u001b[1m605/605\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - accuracy: 0.9858 - loss: 0.0588\n",
      "Epoch 3: val_loss did not improve from 0.03817\n",
      "\u001b[1m605/605\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 124ms/step - accuracy: 0.9858 - loss: 0.0588 - val_accuracy: 0.9847 - val_loss: 0.0473\n",
      "Epoch 4/8\n",
      "\u001b[1m605/605\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - accuracy: 0.9902 - loss: 0.0468\n",
      "Epoch 4: val_loss improved from 0.03817 to 0.03605, saving model to best_model_human_15.keras\n",
      "\u001b[1m605/605\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 141ms/step - accuracy: 0.9902 - loss: 0.0468 - val_accuracy: 0.9907 - val_loss: 0.0360\n",
      "Epoch 5/8\n",
      "\u001b[1m605/605\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - accuracy: 0.9932 - loss: 0.0288\n",
      "Epoch 5: val_loss improved from 0.03605 to 0.01542, saving model to best_model_human_15.keras\n",
      "\u001b[1m605/605\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 143ms/step - accuracy: 0.9932 - loss: 0.0288 - val_accuracy: 0.9946 - val_loss: 0.0154\n",
      "Epoch 6/8\n",
      "\u001b[1m605/605\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - accuracy: 0.9949 - loss: 0.0266\n",
      "Epoch 6: val_loss did not improve from 0.01542\n",
      "\u001b[1m605/605\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 141ms/step - accuracy: 0.9949 - loss: 0.0266 - val_accuracy: 0.9915 - val_loss: 0.0188\n",
      "Epoch 7/8\n",
      "\u001b[1m605/605\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - accuracy: 0.9967 - loss: 0.0157\n",
      "Epoch 7: val_loss did not improve from 0.01542\n",
      "\u001b[1m605/605\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 125ms/step - accuracy: 0.9967 - loss: 0.0157 - val_accuracy: 0.9901 - val_loss: 0.0302\n",
      "Epoch 8/8\n",
      "\u001b[1m605/605\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - accuracy: 0.9963 - loss: 0.0198\n",
      "Epoch 8: val_loss improved from 0.01542 to 0.00959, saving model to best_model_human_15.keras\n",
      "\u001b[1m605/605\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 130ms/step - accuracy: 0.9963 - loss: 0.0198 - val_accuracy: 0.9967 - val_loss: 0.0096\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 41ms/step - accuracy: 0.9969 - loss: 0.0081\n",
      "Validation Accuracy: 0.9967\n"
     ]
    }
   ],
   "source": [
    "# ===== 学習 =====\n",
    "history = model.fit(\n",
    "    [X_left_train, X_right_train, train_ear_left_scaled, train_ear_right_scaled],\n",
    "    y_train,\n",
    "    validation_data=(\n",
    "        [X_left_test, X_right_test, val_ear_left_scaled, val_ear_right_scaled],\n",
    "        y_test\n",
    "    ),\n",
    "    epochs=8,\n",
    "    batch_size=32,\n",
    "    callbacks=[checkpoint],\n",
    "    class_weight={0:1, 1:5}\n",
    ")\n",
    "\n",
    "# ===== 評価 =====\n",
    "loss, acc = model.evaluate(\n",
    "    [X_left_test, X_right_test, val_ear_left_scaled, val_ear_right_scaled],\n",
    "    y_test\n",
    ")\n",
    "print(f\"Validation Accuracy: {acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7dbc1929",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 23:57:43.588 python[36664:3319351] WARNING: AVCaptureDeviceTypeExternal is deprecated for Continuity Cameras. Please use AVCaptureDeviceTypeContinuityCamera and add NSCameraUseContinuityCameraDeviceType to your Info.plist.\n",
      "/Users/moriwataru/.pyenv/versions/3.11.4/lib/python3.11/site-packages/keras/src/models/functional.py:225: UserWarning: The structure of `inputs` doesn't match the expected structure: ['input_layer_2', 'input_layer_3', 'ear_left_input', 'ear_right_input']. Received: the structure of inputs=('*', '*', '*', '*')\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import dlib\n",
    "import numpy as np\n",
    "from scipy.spatial import distance as dist\n",
    "from collections import deque\n",
    "from tensorflow.keras.models import load_model\n",
    "import pygame\n",
    "from playsound3 import playsound\n",
    "\n",
    "# ===== EAR計算関数 =====\n",
    "def eye_aspect_ratio(eye):\n",
    "    A = dist.euclidean(eye[1], eye[5])\n",
    "    B = dist.euclidean(eye[2], eye[4])\n",
    "    C = dist.euclidean(eye[0], eye[3])\n",
    "    ear = (A + B) / (2.0 * C)\n",
    "    return ear\n",
    "\n",
    "# ===== 移動平均クラス =====\n",
    "class MovingAverage:\n",
    "    def __init__(self, size=5):\n",
    "        self.values = deque(maxlen=size)\n",
    "    def update(self, val):\n",
    "        self.values.append(val)\n",
    "        return np.mean(self.values) if self.values else val\n",
    "\n",
    "# ===== モデル読み込み =====\n",
    "model = load_model(\"best_model_human.keras\",safe_mode=False)\n",
    "\n",
    "# ===== Dlib準備 =====\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n",
    "\n",
    "# 目のランドマークインデックス\n",
    "LEFT_EYE_IDX = list(range(36, 42))\n",
    "RIGHT_EYE_IDX = list(range(42, 48))\n",
    "\n",
    "# ===== 移動平均用インスタンス =====\n",
    "ma_left = MovingAverage(size=5)\n",
    "ma_right = MovingAverage(size=5)\n",
    "ma_pred = MovingAverage(size=3)   # モデル予測値\n",
    "\n",
    "pygame.mixer.init()\n",
    "pygame.mixer.music.load(\"kisho.mp3\")\n",
    "\n",
    "drowsy_counter = 0\n",
    "drowsy_threshold = 10\n",
    "alarm_played = False\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = detector(gray, 0)\n",
    "    \n",
    "    for face in faces:\n",
    "        shape = predictor(gray, face)\n",
    "        shape_np = np.zeros((68, 2), dtype=\"int\")\n",
    "        for i in range(68):\n",
    "            shape_np[i] = (shape.part(i).x, shape.part(i).y)\n",
    "        \n",
    "        # 左目\n",
    "        left_eye_points = shape_np[LEFT_EYE_IDX]\n",
    "        right_eye_points = shape_np[RIGHT_EYE_IDX]\n",
    "        \n",
    "        # EAR計算\n",
    "        ear_left = ma_left.update(eye_aspect_ratio(left_eye_points))\n",
    "        ear_right = ma_right.update(eye_aspect_ratio(right_eye_points))\n",
    "\n",
    "        ear_left = scaler_left.transform([[ear_left]])[0][0] * alpha\n",
    "        ear_right = scaler_right.transform([[ear_right]])[0][0] * alpha\n",
    "\n",
    "        \n",
    "        # 左目画像切り出し\n",
    "        lx, ly, lw, lh = cv2.boundingRect(left_eye_points)\n",
    "        left_eye_img = frame[ly:ly+lh, lx:lx+lw]\n",
    "        left_eye_img = cv2.resize(left_eye_img, (64, 64))\n",
    "        \n",
    "        # 右目画像切り出し\n",
    "        rx, ry, rw, rh = cv2.boundingRect(right_eye_points)\n",
    "        right_eye_img = frame[ry:ry+rh, rx:rx+rw]\n",
    "        right_eye_img = cv2.resize(right_eye_img, (64, 64))\n",
    "        \n",
    "        # モデル入力形式に変換\n",
    "        left_eye_img = left_eye_img.astype(\"float32\") / 255.0\n",
    "        right_eye_img = right_eye_img.astype(\"float32\") / 255.0\n",
    "        left_eye_img = np.expand_dims(left_eye_img, axis=0)\n",
    "        right_eye_img = np.expand_dims(right_eye_img, axis=0)\n",
    "        ear_left_input = np.array([[ear_left]], dtype=np.float32)\n",
    "        ear_right_input = np.array([[ear_right]], dtype=np.float32)\n",
    "        \n",
    "        # 予測\n",
    "        pred = model.predict([left_eye_img, right_eye_img, ear_left_input, ear_right_input], verbose=0)[0][0]\n",
    "        pred = ma_pred.update(pred)\n",
    "        # 判定\n",
    "        label = \"Drowsy\" if pred > 0.2 else \"Non Drowsy\"\n",
    "        color = (0, 0, 255) if label == \"Drowsy\" else (0, 255, 0)\n",
    "        \n",
    "        # 警告表示\n",
    "        cv2.putText(frame, f\"{label} ({ear_left:.2f})\", (50, 50),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1.2, color, 2)\n",
    "        if label == \"Drowsy\":\n",
    "            cv2.putText(frame, \"!!! WARNING !!!\", (100, 100),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0, 0, 255), 3)\n",
    "            drowsy_counter += 1\n",
    "        else:\n",
    "            drowsy_counter = 0\n",
    "            # Non Drowsy になったら音楽停止\n",
    "            if alarm_played:\n",
    "                pygame.mixer.music.stop()\n",
    "                alarm_played = False\n",
    "        # 一定時間 Drowsy が続いたら音楽\n",
    "        if drowsy_counter >= drowsy_threshold and not alarm_played:\n",
    "            pygame.mixer.music.play(1) \n",
    "            alarm_played = True\n",
    "    \n",
    "    cv2.imshow(\"Drowsiness Detection\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7fe1f6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save(\"my_best_model.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58812268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Width: 1280.0, Height: 720.0, FPS: 30.0\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d5b974",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
